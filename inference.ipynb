{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e0e7c-ae45-4989-a75d-c351298d13e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tensorflow import keras\n",
    "from keras_bert import get_base_dict, get_model, compile_model, gen_batch_inputs, get_custom_objects\n",
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c781a2-c9e8-48c1-abb1-83e8a1dbd237",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reconstructing the dictonary from training data\n",
    "\n",
    "This step appears necessary so that the validation data and training data share the same UMID --> token mapping. This mapping can vary based on the order of when a sequence appears in the loop, hence need to use the training data to reconstruct it for consistency. Of course, we can always save a constructed dict and load it as well, based on needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c6edc0-c0d9-4adc-83b7-8b1f66406303",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_dir = '/global/cfs/cdirs/m3443/usr/ahuang/tracking-bert/train_data' # The input directory for data\n",
    "\n",
    "data = np.load(f'{input_dir}/inputs/train.npz', allow_pickle=True)\n",
    "seq = data['seq']\n",
    "true_pt = data['true_pt']\n",
    "true_seq = []\n",
    "\n",
    "for i in range(len(seq)):\n",
    "    temp = []\n",
    "    for j in range(len(seq[i])):\n",
    "        temp.append(seq[i][j])\n",
    "    true_seq.append(temp)\n",
    "    \n",
    "sentence_pairs2 = []\n",
    "for i in range(0, len(true_seq)-1, 2):\n",
    "    if true_pt[i] >= true_pt[i+1]:\n",
    "        sentence_pairs2.append([true_seq[i], true_seq[i+1]])\n",
    "    else:\n",
    "        sentence_pairs2.append([true_seq[i+1], true_seq[i]])\n",
    "    \n",
    "# Build token dictionary\n",
    "token_dict = get_base_dict()  # A dict that contains some special tokens\n",
    "for pairs in sentence_pairs2:\n",
    "    for token in pairs[0] + pairs[1]:\n",
    "        if token not in token_dict:\n",
    "            token_dict[token] = len(token_dict)\n",
    "token_list = list(token_dict.keys())  # Used for selecting a random word\n",
    "token_dict_inv = {v: k for k, v in token_dict.items()} # Get the inverse mapping from actual token to UMID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9d68fd-59ca-4384-8d15-24cd9c6bfdc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from process_data import TrackMLReader\n",
    "\n",
    "## Reproduce umid_dict the same way as in process_data\n",
    "data_input_dir = \"/global/cfs/cdirs/m3443/data/trackml-kaggle/train_all\"\n",
    "detector_path = f'detectors.csv'\n",
    "reader = TrackMLReader(data_input_dir, detector_path=detector_path)\n",
    "detector = pd.read_csv(detector_path)\n",
    "\n",
    "detector_umid = np.stack([detector.volume_id, detector.layer_id, detector.module_id], axis=1)\n",
    "umid_dict = {}\n",
    "index = 1\n",
    "for i in detector_umid:\n",
    "    umid_dict[tuple(i)] = index\n",
    "    index += 1\n",
    "\n",
    "## Inverting the umid_dict\n",
    "umid_dict_inv = {v: k for k, v in umid_dict.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d7a5da-4e5b-4a88-a41c-8612031711c5",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Loading test/validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fd3189c-111c-46a1-9e83-993614add435",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = '/global/cfs/cdirs/m3443/usr/ahuang/tracking-bert/train_data' # The input directory for data\n",
    "\n",
    "data = np.load(f'{input_dir}/inputs/val.npz', allow_pickle=True)\n",
    "seq0 = data['seq']\n",
    "pt = data['true_pt']\n",
    "ind = np.flip(np.argsort(pt.flatten()))\n",
    "\n",
    "sort_all = False # If true, the entire data will be sorted in decreasing Pt order\n",
    "sort_pairs = True # If true, each pair will be sorted in decreasing Pt order, but not necessarily the entire dataset\n",
    "\n",
    "if sort_all:\n",
    "    seq = seq0[ind]\n",
    "else:\n",
    "    seq = seq0\n",
    "\n",
    "print('>>> Number of sequences:', len(seq))\n",
    "true_seq = []\n",
    "for i in range(len(seq)):\n",
    "    temp = []\n",
    "    for j in range(len(seq[i])):\n",
    "        temp.append(seq[i][j])\n",
    "    true_seq.append(temp)\n",
    "    \n",
    "sentence_pairs = []\n",
    "if sort_pairs:\n",
    "    for i in range(0, len(true_seq)-1, 2):\n",
    "        if pt[i] >= pt[i+1]:\n",
    "            sentence_pairs.append([true_seq[i], true_seq[i+1]])\n",
    "        else:\n",
    "            sentence_pairs.append([true_seq[i+1], true_seq[i]])\n",
    "else:\n",
    "    for i in range(0, len(true_seq)-1, 2):\n",
    "        sentence_pairs.append([true_seq[i], true_seq[i+1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5578f1c7-30fd-4a28-bf53-28ade8cca98b",
   "metadata": {},
   "source": [
    "## Utils\n",
    "[WARNING] A lot of util functions are here! For future development, consider place some/all of them into a .py file and load them when needed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e4e872c-9dcc-4491-9e3e-b35f34c4b597",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Used for input loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8456eeb-c58b-4a19-8197-3fa34e16c8dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_size = len(sentence_pairs)\n",
    "def test_generator(batch_size=5000, seq_len=10, mask_id=None,\n",
    "                     mask_first=True):\n",
    "    \"\"\"\n",
    "    A generator to generate test inputs.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size : int\n",
    "        The batch size for inference\n",
    "    seq_len : int\n",
    "        The sequence length\n",
    "    mask_id : str\n",
    "        \"first\", \"last\", or anything else, if \"first\" then masking the first\n",
    "        hit in the track, if \"last\" then masking the last, else then randomly\n",
    "        mask one hit in the middle of the track\n",
    "    mask_first : bool\n",
    "        If true, mask one element in track A of input pair [A, B], else then\n",
    "        mask one element in track B\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A batch of inputs in tuples in order of\n",
    "        * inputs: input token array\n",
    "        * outputs: truth array\n",
    "        * masked_seq_len: the length of the sequence masked\n",
    "        * mask_position: the position of the mask\n",
    "        * mlm_truth: truth token for the masked hit\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while True:\n",
    "        yield gen_batch_inputs(\n",
    "                     sentence_pairs[i*batch_size:],\n",
    "                     token_dict,\n",
    "                     token_list,\n",
    "                     batch_size=batch_size,\n",
    "                     seq_len=seq_len,\n",
    "                     #mask_rate=mask_rate, # Not needed since this is not training\n",
    "                     #mask_mask_rate=0., \n",
    "                     #mask_random_rate=0.,\n",
    "                     #swap_sentence_rate=0.,\n",
    "                     #force_mask=True,\n",
    "                     mask_id=mask_id,\n",
    "                     mask_first=mask_first\n",
    "        )\n",
    "        if (i+1) * batch_size >= total_size:\n",
    "            i = 0\n",
    "        else:\n",
    "            i += 1\n",
    "            \n",
    "def test_generator_multimask(batch_size=5000, seq_len=10, mask_n=2,\n",
    "                     mask_first=True):\n",
    "    \"\"\"\n",
    "    A generator to generate test inputs for masking multiple hits in the track.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    batch_size : int\n",
    "        The batch size for inference\n",
    "    seq_len : int\n",
    "        The sequence length\n",
    "    mask_n : int\n",
    "        Number of elements to mask in a sequence\n",
    "    mask_first : bool\n",
    "        If true, mask one element in track A of input pair [A, B], else then\n",
    "        mask one element in track B\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    A batch of inputs in tuples in order of\n",
    "        * inputs: input token array\n",
    "        * outputs: truth array\n",
    "        * masked_seq_len: the length of the sequence masked\n",
    "        * mask_position: the position of the mask\n",
    "        * mlm_truth: truth token for the masked hit\n",
    "    \"\"\"\n",
    "    i = 0\n",
    "    while True:\n",
    "        yield gen_batch_inputs_multimask(\n",
    "                     sentence_pairs[i*batch_size:],\n",
    "                     token_dict,\n",
    "                     token_list,\n",
    "                     batch_size=batch_size,\n",
    "                     seq_len=seq_len,\n",
    "                     #mask_rate=mask_rate,  # Not needed since this is not training\n",
    "                     #mask_mask_rate=0.,\n",
    "                     #mask_random_rate=0.,\n",
    "                     #swap_sentence_rate=0.,\n",
    "                     #force_mask=True,\n",
    "                     mask_n=mask_n,\n",
    "                     mask_first=mask_first\n",
    "        )\n",
    "        if (i+1) * batch_size >= total_size:\n",
    "            i = 0\n",
    "        else:\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9246aba-c2ed-4ac2-840b-28b5fa6eb7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(test_seq):\n",
    "    \"\"\"\n",
    "    Tokenize an input sequence into inputs needed for predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    test_seq : arr\n",
    "        The input sequence, in the form of a tuple\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    test_input : arr\n",
    "        Test input to feed into a model for prediction\n",
    "    truth : arr\n",
    "        The truth array\n",
    "    mask_position : int or arr\n",
    "        The index for the masked hit(s)\n",
    "    mask_seq_len : int\n",
    "        The sequence length of the masked array\n",
    "    \"\"\"\n",
    "    [indices, seg, masks], [truth_seq, _], mask_seq_len, mask_position, truth = test_seq\n",
    "    test_input = [indices, seg, masks]\n",
    "    truth = list(truth) \n",
    "    return test_input, truth, mask_position, mask_seq_len # mask indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017c57c2-ec39-4e87-a6ee-423b397b53ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_batch_inputs(sentence_pairs,\n",
    "                     token_dict,\n",
    "                     token_list,\n",
    "                     batch_size=5000,\n",
    "                     seq_len=512,\n",
    "                     mask_rate=0.15,\n",
    "                     mask_id=None,\n",
    "                     mask_first=True):\n",
    "    \"\"\"Generate a batch of inputs and outputs for testing.\n",
    "\n",
    "    :param sentence_pairs: A list of pairs containing lists of tokens.\n",
    "    :param token_dict: The dictionary containing special tokens.\n",
    "    :param token_list: A list containing all tokens.\n",
    "    :param seq_len: Length of the sequence.\n",
    "    :param mask_rate: The rate of choosing a token for prediction.\n",
    "    :param mask_mask_rate: The rate of replacing the token to `TOKEN_MASK`.\n",
    "    :param mask_random_rate: The rate of replacing the token to a random word.\n",
    "    :param swap_sentence_rate: The rate of swapping the second sentences.\n",
    "    :param force_mask: At least one position will be masked.\n",
    "    :return: All the inputs and outputs.\n",
    "    \"\"\"\n",
    "    TOKEN_PAD = ''  # Token for padding\n",
    "    TOKEN_UNK = '[UNK]'  # Token for unknown words\n",
    "    TOKEN_CLS = '[CLS]'  # Token for classification\n",
    "    TOKEN_SEP = '[SEP]'  # Token for separation\n",
    "    TOKEN_MASK = '[MASK]'  # Token for masking\n",
    "    batch_size = min(batch_size, len(sentence_pairs))\n",
    "    base_dict = get_base_dict()\n",
    "    unknown_index = token_dict[TOKEN_UNK]\n",
    "    nsp_outputs = np.zeros((batch_size,))\n",
    "    \n",
    "    # Generate MLM\n",
    "    token_inputs, segment_inputs, masked_inputs = [], [], []\n",
    "    mlm_outputs = []\n",
    "    masked_seq_len = []\n",
    "    mask_position = []\n",
    "    mlm_truth = []\n",
    "    for i in range(batch_size):\n",
    "        first, second = sentence_pairs[i]\n",
    "        segment_inputs.append(([0] * (len(first) + 2) + [1] * (seq_len - (len(first) + 2)))[:seq_len])\n",
    "        tokens = [TOKEN_CLS] + first + [TOKEN_SEP] + second + [TOKEN_SEP]\n",
    "        tokens = tokens[:seq_len]\n",
    "        tokens += [TOKEN_PAD] * (seq_len - len(tokens))\n",
    "        token_input, masked_input, mlm_output = [], [], []\n",
    "\n",
    "        for token in tokens:\n",
    "            mlm_output.append(token_dict.get(token, unknown_index))\n",
    "            masked_input.append(0)\n",
    "            token_input.append(token_dict.get(token, unknown_index))\n",
    "            \n",
    "        if mask_first:\n",
    "            start, end, length = 1, len(first), len(first)\n",
    "        else:\n",
    "            start, end, length = len(first)+2, len(first)+len(second)+1, len(second)\n",
    "            \n",
    "        if mask_id == 'first':\n",
    "            mid = start\n",
    "        elif mask_id == 'last':\n",
    "            mid = end\n",
    "        else:\n",
    "            mid = np.random.randint(start+1, end)\n",
    "        \n",
    "        masked_input[mid] = 1\n",
    "        token_input[mid] = token_dict[TOKEN_MASK] # NOTE: Changing the test input to the [MASK] token, effect unclear\n",
    "        mask_position.append(mid)\n",
    "        masked_seq_len.append(length)\n",
    "        token_inputs.append(token_input)\n",
    "        masked_inputs.append(masked_input)\n",
    "        mlm_outputs.append(mlm_output)\n",
    "        mlm_truth.append(mlm_output[mid])\n",
    "    inputs = [np.asarray(x) for x in [token_inputs, segment_inputs, masked_inputs]]\n",
    "    outputs = [np.asarray(np.expand_dims(x, axis=-1)) for x in [mlm_outputs, nsp_outputs]]\n",
    "    \n",
    "    return inputs, outputs, masked_seq_len, mask_position, mlm_truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aab3a69-3c16-427f-a35c-e63a0c2bd489",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def gen_batch_inputs_multimask(sentence_pairs,\n",
    "                     token_dict,\n",
    "                     token_list,\n",
    "                     batch_size=5000,\n",
    "                     seq_len=512,\n",
    "                     mask_rate=0.15,\n",
    "                     mask_n=2,\n",
    "                     mask_first=True):\n",
    "    \"\"\"Generate a batch of inputs and outputs for testing.\n",
    "    Multiple masks can be applied based on mask_n.\n",
    "\n",
    "    :param sentence_pairs: A list of pairs containing lists of tokens.\n",
    "    :param token_dict: The dictionary containing special tokens.\n",
    "    :param token_list: A list containing all tokens.\n",
    "    :param seq_len: Length of the sequence.\n",
    "    :param mask_rate: The rate of choosing a token for prediction.\n",
    "    :param mask_mask_rate: The rate of replacing the token to `TOKEN_MASK`.\n",
    "    :param mask_random_rate: The rate of replacing the token to a random word.\n",
    "    :param swap_sentence_rate: The rate of swapping the second sentences.\n",
    "    :param force_mask: At least one position will be masked.\n",
    "    :return: All the inputs and outputs.\n",
    "    \"\"\"\n",
    "    TOKEN_PAD = ''  # Token for padding\n",
    "    TOKEN_UNK = '[UNK]'  # Token for unknown words\n",
    "    TOKEN_CLS = '[CLS]'  # Token for classification\n",
    "    TOKEN_SEP = '[SEP]'  # Token for separation\n",
    "    TOKEN_MASK = '[MASK]'  # Token for masking\n",
    "    batch_size = min(batch_size, len(sentence_pairs))\n",
    "    base_dict = get_base_dict()\n",
    "    unknown_index = token_dict[TOKEN_UNK]\n",
    "    nsp_outputs = np.zeros((batch_size,))\n",
    "    \n",
    "    # Generate MLM\n",
    "    token_inputs, segment_inputs, masked_inputs = [], [], []\n",
    "    mlm_outputs = []\n",
    "    masked_seq_len = []\n",
    "    mask_position = []\n",
    "    mlm_truth = []\n",
    "    for i in range(batch_size):\n",
    "        first, second = sentence_pairs[i]\n",
    "        segment_inputs.append(([0] * (len(first) + 2) + [1] * (seq_len - (len(first) + 2)))[:seq_len])\n",
    "        tokens = [TOKEN_CLS] + first + [TOKEN_SEP] + second + [TOKEN_SEP]\n",
    "        tokens = tokens[:seq_len]\n",
    "        tokens += [TOKEN_PAD] * (seq_len - len(tokens))\n",
    "        token_input, masked_input, mlm_output = [], [], []\n",
    "\n",
    "        for token in tokens:\n",
    "            mlm_output.append(token_dict.get(token, unknown_index))\n",
    "            masked_input.append(0)\n",
    "            token_input.append(token_dict.get(token, unknown_index))\n",
    "            \n",
    "        if mask_first:\n",
    "            start, end, length = 1, len(first), len(first)\n",
    "        else:\n",
    "            start, end, length = len(first)+2, len(first)+len(second)+1, len(second)\n",
    "            \n",
    "        \n",
    "        mids = np.random.randint(start, end, mask_n)\n",
    "        mlm_truth_temp = []\n",
    "        for mid in mids:\n",
    "            token_input[mid] = token_dict[TOKEN_MASK] # NOTE: Changing the test input to the [MASK] token, effect unclear\n",
    "            masked_input[mid] = 1\n",
    "            mask_position.append(mid)\n",
    "            mlm_truth_temp.append(mlm_output[mid])\n",
    "        masked_seq_len.append(length)\n",
    "        token_inputs.append(token_input)\n",
    "        masked_inputs.append(masked_input)\n",
    "        mlm_outputs.append(mlm_output)\n",
    "        mlm_truth.append(mlm_truth_temp)\n",
    "    inputs = [np.asarray(x) for x in [token_inputs, segment_inputs, masked_inputs]]\n",
    "    outputs = [np.asarray(np.expand_dims(x, axis=-1)) for x in [mlm_outputs, nsp_outputs]]\n",
    "\n",
    "    return inputs, outputs, masked_seq_len, mask_position, mlm_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e230b85-2a59-402b-8480-b3d1d27d6c34",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Used for post-processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1cede41-8866-4623-9285-738fad91033f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def dist(t, p):\n",
    "    \"\"\"\n",
    "    Calculating the Euclidean distance between the center of the predicted detector element\n",
    "    and the true detector element.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t : int\n",
    "        The truth index for the detector module\n",
    "    p : int\n",
    "        The predicted index for the detector module\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    The distance between t and p\n",
    "    \"\"\"\n",
    "    r = np.array([detector.loc[t]['cx'], detector.loc[t]['cy'],detector.loc[t]['cz']])\n",
    "    rp = np.array([detector.loc[p]['cx'], detector.loc[p]['cy'],detector.loc[p]['cz']])\n",
    "    return np.linalg.norm(r-rp)\n",
    "\n",
    "def locate_mod(t):\n",
    "    \"\"\"\n",
    "    Locate a module based on the index.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    t : int\n",
    "        UMID of the module\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    r, z : float, float\n",
    "        The r, z coordinate of the detector module\n",
    "    \"\"\"\n",
    "    V, L, M = umid_dict_inv[t]\n",
    "    row = detector.loc[detector.volume_id==V].loc[detector.layer_id==L].loc[detector.module_id==M]\n",
    "    r = np.sqrt(row['cx']**2 + row['cy']**2)\n",
    "    z = row['cz']\n",
    "    return float(r), float(z)\n",
    "\n",
    "def get_track_len(test_trk, n_mask=1):\n",
    "    \"\"\"\n",
    "    Get the length of the test_trk\n",
    "    \"\"\"\n",
    "    seq = test_trk[0][0]\n",
    "    index = 0\n",
    "    j = 0\n",
    "    for i in seq:\n",
    "        if i == 3:\n",
    "            index = j\n",
    "            break\n",
    "        j += 1\n",
    "    return [j-1]*n_mask\n",
    "\n",
    "def calc_acc(D):\n",
    "    \"\"\"\n",
    "    Calculating the accuracy of prediction, given their predicted distance D.\n",
    "    Returns the accuracy of D = 0 and D < 100\n",
    "    \"\"\"\n",
    "    acc_0 = len([i for i in D if i == 0]) / len(D)\n",
    "    acc_100 = len([i for i in D if i <= 100]) / len(D)\n",
    "    return acc_0, acc_100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98842a3c-c4fe-4ac5-b0ea-8d8ba4780493",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd48cb74-c93b-43d2-b9be-351f08184366",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_D(D, ax, right_bound=1e4):\n",
    "    \"\"\"\n",
    "    Plot the distribution (pdf and cdf) of distances in histogram.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    D : arr\n",
    "        Distances calculated\n",
    "    ax : plt.axes\n",
    "        An axis to plot the figure on, can be \n",
    "        initialized in fig, ax = plt.subplots()\n",
    "    right_bound : float\n",
    "        The upper bound for the x axis for better visualization\n",
    "    \"\"\"\n",
    "    weights = np.ones_like(D) / len(D)\n",
    "    rang = (min(D), max(D))\n",
    "    bin_wid = 10\n",
    "    rang_adj = (-10, min(max(D)+50, right_bound))\n",
    "    nbins = int(rang_adj[1] - rang_adj[0]) // bin_wid + 1\n",
    "    \n",
    "    count, bins_count, _ = ax[0].hist(D, bins=nbins, range=rang_adj, weights=weights, align='mid')#, histtype='step', lw=2)\n",
    "    cdf = np.cumsum(count)\n",
    "    ax[1].plot(bins_count[1:], cdf)\n",
    "    ax[0].set_xlabel('Distance (mm)')\n",
    "    ax[1].set_xlabel('Distance (mm)')\n",
    "    ax[0].set_ylabel(f'Normalized Frequency / {bin_wid:.2f} mm')\n",
    "    ax[0].set_title(f'Normalized Distribution, Accuracy: {calc_acc(D)[0] *100:.2f}%')\n",
    "    ax[1].set_title(f'Cumulative Distribution')\n",
    "    ax[0].grid()\n",
    "    ax[1].grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06dbe768-eabc-4393-9deb-0e3a469506c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_D_maskPos(D_first, D_mid, D_last, ax):\n",
    "    \"\"\"\n",
    "    Plot the distances vs the mask positions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    D_first : arr\n",
    "        The distances for predictions on masked hits\n",
    "        that the FIRST element in a sequence\n",
    "    D_mid : arr\n",
    "        The distances for predictions on masked hits\n",
    "        that in the MIDDLE of a sequence\n",
    "    D_last : arr\n",
    "        The distances for predictions on masked hits\n",
    "        that the LAST element in a sequence\n",
    "    ax : plt.axes\n",
    "        An axis to plot the figure on, can be \n",
    "        initialized in fig, ax = plt.subplots()\n",
    "    \"\"\"\n",
    "    x = np.arange(3)  # the label locations\n",
    "    width = 0.25  # the width of the bars\n",
    "    multiplier = 1\n",
    "    acc0, acc100 = [], []\n",
    "    for i in [D_first, D_last, D_mid]:\n",
    "        a, b = calc_acc(i)\n",
    "        acc0.append(a)\n",
    "        acc100.append(b)\n",
    "\n",
    "    rec1 = ax.bar(x, acc0, width, alpha=0.6, label='D = 0 mm')\n",
    "    ax.bar_label(rec1, padding=2, fmt='%.2f')\n",
    "    rec2 = ax.bar(x+width * multiplier, acc100, width, alpha=0.6, label='D < 100 mm')\n",
    "    ax.bar_label(rec2, padding=2, fmt='%.2f')\n",
    "\n",
    "    names = ['First', 'Mid', 'Last']\n",
    "    ax.set_xticks(x + width/2, names)\n",
    "    ax.set_xlabel('Mask Position')\n",
    "    ax.set_ylabel('Proportion of Tracks')\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title('Influence of Mask Position')\n",
    "    ax.grid()\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb496a7-a992-4dd3-aa45-96f999e76ca1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def refractor(D, L):\n",
    "    \"\"\"\n",
    "    Calculate the mean and standard deviation for the distances\n",
    "    of each track length. For instance, suppose the tracks with length\n",
    "    4 have validation distances [100, 200], and tracks with length 5\n",
    "    have distances [100, 500], then a dict of {4: [100, 200], 5: [100, 500]}\n",
    "    will be formed and mean and std will be calculated for each track length.\n",
    "    The return will be ([150, 300], [std([100,200]), std([100, 500])])\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    D : arr\n",
    "        Distances between predicted modules and truth modules\n",
    "    L : arr\n",
    "        The corresponding length of the tracks used for inference\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mean : arr\n",
    "        The mean of respective distances\n",
    "    std : arr\n",
    "        The std of respective distances\n",
    "    \"\"\"\n",
    "    base = {}\n",
    "    for i in range(min(L), max(L)+1):\n",
    "        base[i] = []\n",
    "    for d, l in zip(D, L):\n",
    "        base[l].append(d)\n",
    "    mean, std = [], []\n",
    "    for i in base:\n",
    "        m = np.mean(base[i])\n",
    "        s = np.std(base[i])\n",
    "        mean.append(m)\n",
    "        std.append(s)\n",
    "    return mean, std\n",
    "\n",
    "def plot_D_len(D, trk_len, ax):\n",
    "    \"\"\"\n",
    "    Plot the distance w.r.t. different length of the tracks.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    D : arr\n",
    "        The distances\n",
    "    trk_len : arr\n",
    "        The length of the respective tracks\n",
    "    ax : plt.axes\n",
    "        The axis to plot on\n",
    "    \"\"\"\n",
    "    ax.plot(trk_len, D, '.', markersize=5, label='Data')\n",
    "    x = list(range(min(trk_len), max(trk_len)+1))\n",
    "    mean, std = refractor(D, trk_len)\n",
    "    ax.errorbar(x, mean, yerr=std, fmt='o', ecolor='r', capsize=2, label='Mean distance')\n",
    "    ax.set_xticks(x)\n",
    "    ax.legend()\n",
    "    ax.grid()\n",
    "    ax.set_xlabel('Sequence Length')\n",
    "    ax.set_ylabel('Distance (mm)')\n",
    "    ax.set_title('Influence of Sequence Length')\n",
    "    \n",
    "def refractor2(D, L):\n",
    "    \"\"\"\n",
    "    Calculate the mean and standard deviation for the distances\n",
    "    of each detector module. For instance, suppose the detector module\n",
    "    4 has validation distances [100, 200], and detector module 5\n",
    "    has distances [100, 500], then a dict of {4: [100, 200], 5: [100, 500]}\n",
    "    will be formed and mean and std will be calculated for each track length.\n",
    "    The return will be ([150, 300], [std([100,200]), std([100, 500])], [4,5], dict)\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    D : arr\n",
    "        Distances between predicted modules and truth modules\n",
    "    L : arr\n",
    "        The corresponding UMID\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    mean : arr\n",
    "        The mean of respective distances\n",
    "    std : arr\n",
    "        The std of respective distances\n",
    "    UMID : arr\n",
    "        The list of all UMID encountered in this function\n",
    "    base : dict\n",
    "        The dictionary {UMID: distances}\n",
    "    \"\"\"\n",
    "    base = {}\n",
    "    for l in L:\n",
    "        if l not in base:\n",
    "            base[l] = []\n",
    "    for d, l in zip(D, L):\n",
    "        base[l].append(d)\n",
    "    mean, std = [], []\n",
    "    for i in base:\n",
    "        m = np.mean(base[i])\n",
    "        s = np.std(base[i])\n",
    "        mean.append(m)\n",
    "        std.append(s)\n",
    "    return mean, std, list(base.keys()), base\n",
    "    \n",
    "def plot_D_mod(D, mod, ax):\n",
    "    \"\"\"\n",
    "    Plot the different distances w.r.t. different detector module.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    D : arr\n",
    "        The distances\n",
    "    mod : arr\n",
    "        An array of respective UMIDs\n",
    "    ax : plt.axes\n",
    "        The axis to plot on\n",
    "    \"\"\"\n",
    "    mean, std, UMID, base = refractor2(D, mod)\n",
    "    acc0, acc100 = [], []\n",
    "    for i in base:\n",
    "        a, b = calc_acc(base[i])\n",
    "        acc0.append(a)\n",
    "        acc100.append(b)\n",
    "    x = np.array(UMID)  # the label locations\n",
    "    width = 0.5  # the width of the bars\n",
    "    multiplier = 1\n",
    "\n",
    "    rec1 = ax[0].bar(x, acc0, width, label='D = 0 mm') # Bar plot for accuracy vs UMID\n",
    "    ax[0].legend()\n",
    "    ax[0].grid()\n",
    "    ax[0].set_xlabel('Unique Module ID')\n",
    "    ax[0].set_ylabel('Accuracy')\n",
    "    ax[0].set_title('Accuracy vs UMID')\n",
    "    \n",
    "    r, z = [], []\n",
    "    for umid in UMID:\n",
    "        r0, z0 = locate_mod(umid) # use the r-z plane\n",
    "        r.append(r0)\n",
    "        z.append(z0)\n",
    "    im = ax[1].scatter(z, r, s=3, c=acc0)#, cmap='Reds') # scatter plot, color is accuracy\n",
    "    ax[1].set_xlabel('z (mm)')\n",
    "    ax[1].set_ylabel('r (mm)')\n",
    "    plt.colorbar(im, ax=ax[1])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6511e9a-d7a4-455a-8ed2-94eab10273c7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Make predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9d91776-f934-49d9-a083-2d43355ab5bc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pred(model, mid, mask_first, n_sample=3000, seq_len=10, batch_size=1):\n",
    "    \"\"\"\n",
    "    Make predictions.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model : keras.Model\n",
    "        The model loaded, used to perform the inference\n",
    "    mid : int\n",
    "        A specific mask index\n",
    "    mask_first : bool\n",
    "        If true, mask one element in track A of input pair [A, B], else then\n",
    "        mask one element in track B\n",
    "    n_sample : int\n",
    "        Number of samples for inference\n",
    "    seq_len : int\n",
    "        Sequence length\n",
    "    batch_size : int\n",
    "        Batch size\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    D : arr\n",
    "        The Euclidean distance between the center of the predicted detector element\n",
    "        and the true detector element\n",
    "    m_id : arr\n",
    "        The array of indices for the masked hits\n",
    "    trk_len : arr\n",
    "        Length of the sequence that has one or more masks\n",
    "    pred : arr\n",
    "        The actual prediction, in UMIDs\n",
    "    truth : arr\n",
    "        The truth, in UMIDs\n",
    "    \"\"\"\n",
    "        \n",
    "    truth = []\n",
    "    pred = []\n",
    "    m_id = []\n",
    "    trk_len = []\n",
    "    test_seq = test_generator(batch_size=batch_size, seq_len=seq_len,\n",
    "                              mask_id=mid, mask_first=mask_first)\n",
    "    for i in trange(n_sample, desc='inference'):\n",
    "        \n",
    "        ## Loading input sequence in batches\n",
    "        seq = next(test_seq)\n",
    "        test_trk, t, mask_i, mask_seq_len = tokenize(seq)\n",
    "        m_id += mask_i\n",
    "        trk_len += mask_seq_len\n",
    "        \n",
    "        ## Doing inferences\n",
    "        predicts = model.predict(test_trk)[0].argmax(axis=-1) # [MSK, NSP], [0] gives a multi-class classification score for all possible words, argmax gives the most likely word\n",
    "                                                              # shape = (batch_size, seq_len, n_words) --> outputs a list of most likely words with shape (batch_size, len_seq)\n",
    "        ind = list(zip(list(range(len(predicts))), mask_i)) # FIXME: This is currently in an ugly list form to ensure the indices are correct, need to \n",
    "                                                            # change when applied to larger datasets for faster inference.\n",
    "        p = [predicts[tuple(i)] for i in ind]\n",
    "        truth += t\n",
    "        pred += p\n",
    "    \n",
    "    ## Mapping the tokens back to UMID and calculate distance\n",
    "    truth, pred = [token_dict_inv[i] for i in truth], [token_dict_inv[i] for i in pred] #\n",
    "    D = []\n",
    "    for i in trange(len(truth), desc='calc dist'):\n",
    "        if type(truth[i]) == str:\n",
    "            continue\n",
    "        D.append(dist(truth[i], pred[i]))\n",
    "    \n",
    "    return D, m_id, trk_len, pred, truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b17d9d2-44ce-40df-8ad4-79c7cc91f02d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_results(model_name, n_sample=3000, seq_len=10, \n",
    "                right_bound=1e4, batch_size=1, mode='split'):\n",
    "    \"\"\"\n",
    "    A compiled function to get the results. Will plot the following:\n",
    "        * distribution of distances (pdf and cdf)\n",
    "        * accuracy vs mask position\n",
    "        * distance vs sequence length\n",
    "        * accuracy vs UMID\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    model_name : str\n",
    "        The path for to the saved model for inference\n",
    "    n_sample : int\n",
    "        The number of batches to use\n",
    "    seq_len : int\n",
    "        The total length of the input pair\n",
    "    right_bound : float\n",
    "        The upper bound for the x axis for better visualization\n",
    "    batch_size : int\n",
    "        The batch size\n",
    "    mode : str\n",
    "        If \"first_only\", then only do inference on the first track in\n",
    "        each input pair [A, B]; else then do inference on each track \n",
    "        in the pair separately\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    D : arr\n",
    "        Distances, in a flat array\n",
    "    D_all : arr\n",
    "        Distances in ndarray, preserving the structure to identify the\n",
    "        correspondance between masked index and the respective distances\n",
    "    L : arr\n",
    "        Length of each sequence in prediction\n",
    "    P : arr\n",
    "        The sequence of predicted UMIDs\n",
    "    T : arr\n",
    "        The sequence of truth UMIDs\n",
    "    \"\"\"\n",
    "    \n",
    "    ## Load model\n",
    "    model = load_model(model_name, custom_objects=get_custom_objects())\n",
    "    \n",
    "    D, D_all, trk_len, predi = [], [], [], []\n",
    "    D1, D2, L1, L2, P1, P2, T1, T2 = [], [], [], [], [], [], [], []\n",
    "    D1_all, D2_all = [], []\n",
    "    \n",
    "    ## Do inference for masking the first, mid, and last hit of each track\n",
    "    for mid in ['first', 'mid', 'last']:\n",
    "        d1, m, l1, p1, t1 = pred(model, mid, True, n_sample, seq_len=seq_len, \n",
    "                                 batch_size=batch_size)\n",
    "        \n",
    "        D1 += d1\n",
    "        L1 += l1\n",
    "        P1 += p1\n",
    "        T1 += t1\n",
    "        D1_all.append(d1)\n",
    "        \n",
    "        if mode != 'first_only':\n",
    "            d2, m, l2, p2, t2 = pred(model, mid, False, n_sample, seq_len=seq_len, \n",
    "                                     batch_size=batch_size)\n",
    "            D2 += d2\n",
    "            L2 += l2\n",
    "            P2 += p2\n",
    "            T2 += t2\n",
    "            D2_all.append(d2)\n",
    "    \n",
    "    ## Make plots\n",
    "    if mode != 'first_only':\n",
    "        for D, D_all, trk_len in [(D1, D1_all, L1), (D2, D2_all, L2)]:\n",
    "            fig, ax = plt.subplots(figsize=(8,8), ncols=2, nrows=2)\n",
    "            plot_D(D, ax[0], right_bound=right_bound)\n",
    "            plot_D_maskPos(*D_all, ax[1][0])\n",
    "            plot_D_len(D, trk_len, ax[1][1])\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        fig, ax = plt.subplots(figsize=(6*2,6*2), nrows=2, ncols=2)\n",
    "        plot_D_mod(D1, T1, ax[0])\n",
    "        ax[0][0].set_title('Accuracy vs UMID (Inference on First Track in the Pair)')\n",
    "        plot_D_mod(D2, T2, ax[1])\n",
    "        ax[1][0].set_title('Accuracy vs UMID (Inference on Second Track in the Pair)')\n",
    "        plt.show()\n",
    "        return (D1,D2), (D1_all,D2_all), (L1, L2), (P1,P2), (T1,T2)\n",
    "    else:\n",
    "        D = D1 + D2\n",
    "        D_all = D1_all + D2_all\n",
    "        trk_len = L1 + L2\n",
    "        fig, ax = plt.subplots(figsize=(8,8), ncols=2, nrows=2)\n",
    "        plot_D(D, ax[0], right_bound=right_bound)\n",
    "        plot_D_maskPos(*D_all, ax[1][0])\n",
    "        plot_D_len(D, trk_len, ax[1][1])\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(6*2,6), ncols=2)\n",
    "        plot_D_mod(D, T1+T2, ax)\n",
    "    return D, D_all, trk_len, predi, T1+T2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f04fc8ac-4c6b-4349-8ba4-e916c0f718b9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def pred_multimask(model, n, mask_first, n_sample=3000, seq_len=10, batch_size=1):\n",
    "    \"\"\"\n",
    "    Make prediction for the case when multiple hits are masked in a sequence. See\n",
    "    the function **pred** for details. They only differ in this function has an input\n",
    "    *n*, which denotes the number of hits to mask.\n",
    "    \"\"\"\n",
    "    truth = []\n",
    "    pred = []\n",
    "    m_id = []\n",
    "    trk_len = []\n",
    "    test_seq = test_generator_multimask(batch_size=batch_size, seq_len=seq_len,\n",
    "                              mask_n=n, mask_first=mask_first)\n",
    "    \n",
    "    for i in trange(n_sample, desc='inference'):\n",
    "        seq = next(test_seq)\n",
    "        test_trk, t, mask_i, mask_seq_len = tokenize(seq)#, next(truth_seq))\n",
    "    \n",
    "        m_id += mask_i\n",
    "        trk_len += mask_seq_len\n",
    "        \n",
    "        predicts = model.predict(test_trk)[0].argmax(axis=-1)#.tolist() # [MSK, NSP], [0] gives a multi-class classification score for all possible words, argmax gives the most likely word\n",
    "                                                                        # shape = (batch_size, seq_len, n_words) --> outputs a list of most likely words with shape (batch_size, len_seq)\n",
    "        ind1 = np.concatenate([[i]*n for i in range(len(predicts))], axis=0)\n",
    "        ind = list(zip(ind1, mask_i))\n",
    "        p = [predicts[tuple(i)] for i in ind]\n",
    "        truth += t\n",
    "        pred += p\n",
    "    truth = np.array(truth).flatten()\n",
    "    truth, pred = [token_dict_inv[i] for i in truth], [token_dict_inv[i] for i in pred]\n",
    "    \n",
    "    D = []\n",
    "    for i in trange(len(truth), desc='calc dist'):\n",
    "        if type(truth[i]) == str:\n",
    "            continue\n",
    "        D.append(dist(truth[i], pred[i]))\n",
    "\n",
    "    return D, m_id, trk_len, pred, truth\n",
    "\n",
    "def get_results_multimask(model_name, n=1, n_sample=3000, seq_len=10,  \n",
    "                right_bound=1e4, batch_size=1, mode='split', plot=False):\n",
    "    \"\"\"\n",
    "    A compiled function to get the results when multiple hits are masked.\n",
    "    See **get_results** for more details. They only differ in that this\n",
    "    function takes in an argument *n* denoting the number of hits to mask.\n",
    "    Due to the multiple masks in a sequence, the only plots that can be\n",
    "    made reasonably is the distribution of predicted distances.\n",
    "    \"\"\"\n",
    "    model = load_model(model_name, custom_objects=get_custom_objects())\n",
    "    \n",
    "    \n",
    "    D, D_all, trk_len, predi = [], [], [], []\n",
    "    D1, D2, L1, L2, P1, P2, T1, T2 = [], [], [], [], [], [], [], []\n",
    "    D1_all, D2_all = [], []\n",
    "    \n",
    "    \n",
    "    d1, m, l1, p1, t1 = pred_multimask(model, n, True, n_sample, seq_len=seq_len, \n",
    "                             batch_size=batch_size)\n",
    "\n",
    "    D1 += d1\n",
    "    L1 += l1\n",
    "    P1 += p1\n",
    "    T1 += t1\n",
    "    D1_all.append(d1)\n",
    "\n",
    "\n",
    "    if mode != 'first_only':\n",
    "        d2, m, l2, p2, t2 = pred_multimask(model, n, False, n_sample, seq_len=seq_len, \n",
    "                                 batch_size=batch_size)\n",
    "        D2 += d2\n",
    "        L2 += l2\n",
    "        P2 += p2\n",
    "        T2 += t2\n",
    "        D2_all.append(d2)\n",
    "    \n",
    "    if mode != 'first_only':\n",
    "        if plot:\n",
    "            for D, D_all, trk_len in [(D1, D1_all, L1), (D2, D2_all, L2)]:\n",
    "                fig, ax = plt.subplots(figsize=(8,4), ncols=2)\n",
    "                plot_D(D, ax, right_bound=right_bound)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "        fig, ax = plt.subplots(figsize=(6*2,6*2), nrows=2, ncols=2)\n",
    "        plot_D_mod(D1, T1, ax[0])\n",
    "        ax[0][0].set_title('Accuracy vs UMID (Inference on First Track in the Pair)')\n",
    "        plot_D_mod(D2, T2, ax[1])\n",
    "        ax[1][0].set_title('Accuracy vs UMID (Inference on Second Track in the Pair)')\n",
    "        plt.show()\n",
    "        \n",
    "        return (D1,D2), (D1_all,D2_all), (L1, L2), (P1,P2), (T1,T2)\n",
    "    else:\n",
    "        D = D1 + D2\n",
    "        D_all = D1_all + D2_all\n",
    "        trk_len = L1 + L2\n",
    "        \n",
    "    return D, D_all, trk_len, predi, T1+T2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd25d227-7637-42aa-83c3-84f334808f22",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Making some plots on dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19632923-d3e6-4995-8e15-7630a2939674",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "R, Z, X, Y = [], [], [], []\n",
    "j = 0\n",
    "for i in detector.index:\n",
    "    row = detector.loc[i]\n",
    "    r = np.sqrt(row['cx']**2 + row['cy']**2)\n",
    "    x = row['cx']\n",
    "    y = row['cy']\n",
    "    z = row['cz']\n",
    "    R.append(r)\n",
    "    Z.append(z)\n",
    "    X.append(x)\n",
    "    Y.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56dd83b-90db-4655-9931-24ef24b7b535",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(6,6))\n",
    "ax = plt.axes(projection='3d')\n",
    "ax.scatter(X, Y, Z, marker='.', s=3)\n",
    "ax.set_xlabel('x (mm)')\n",
    "ax.set_ylabel('y (mm)')\n",
    "ax.set_zlabel('z (mm)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4e8685-2cf5-42c9-b54b-e304390da7e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "R = []\n",
    "Z = []\n",
    "for m in token_list[5:]:\n",
    "    r, z = locate_mod(m)\n",
    "    R.append(r)\n",
    "    Z.append(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c26a135-efbd-499a-9129-ab029e595afc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_m = []\n",
    "for r, z in zip(R, Z):\n",
    "    if (r, z) in all_m:\n",
    "        #print(\"bad\")\n",
    "        pass\n",
    "    else:\n",
    "        all_m.append((r, z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42574cc2-21e3-42b1-b867-ee0a9da1a779",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.plot(Z, R, 'o', markersize=2)\n",
    "plt.xlabel('z (mm)')\n",
    "plt.ylabel('r (mm)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f03c9c11-c48c-47fd-80f6-73df237f5336",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ca69f74-e229-43f1-a63b-4b7f720e6f5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "D, m_id, trk_len, predi, truth = get_results('/global/cfs/cdirs/m3443/usr/ahuang/tracking-bert/test_model14', n_sample=2, \n",
    "                                             seq_len=19, right_bound=1000, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ff6c5a-57e5-43fe-85aa-349e57d528d4",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "D, m_id, trk_len, predi, truth = get_results('/global/cfs/cdirs/m3443/usr/ahuang/tracking-bert/test_model13', n_sample=2, \n",
    "                                             seq_len=19, right_bound=1000, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28ae79c-c156-4bbb-9c47-781555c72112",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "D, m_id, trk_len, predi, truth = get_results('test_model12', n_sample=2, \n",
    "                                             seq_len=19, right_bound=1000, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f329352-08f8-4503-b5fd-6a46c19d7100",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "D, m_id, trk_len, predi, truth = get_results('test_model9', n_sample=2, mode='first_only',\n",
    "                                             seq_len=10, right_bound=1000, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42dbfff7-f49d-438a-8880-b3776d959541",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "D, m_id, trk_len, predi, truth = get_results('test_model10', n_sample=2, mode='first_only',\n",
    "                                             seq_len=10, right_bound=1000, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b635a9c-c169-4c8f-989a-6115cfd1e03a",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "D, m_id, trk_len, predi, truth = get_results('test_model11', n_sample=2, mode='first_only',\n",
    "                                             seq_len=10, right_bound=1000, batch_size=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db07e6f-2b56-4366-9c11-accd29ae7535",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = load_model('test_model10', custom_objects=get_custom_objects()) # A demo that a model can be load in this way using model path\n",
    "                                                                        # custum_objects is needed to avoid loading errors\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6ab8f3-307c-4480-9282-3c556d34e0d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
